### NOTES ON THE FW_survey PAPER (while i buy a fucking piece of paper)

### keywords
First-order methods · Projection-free methods · Structured optimization ·
Conditional gradient · Sparse optimization

### key terms i don't know but should:
- pareto
- simplex
- cardinality
- first order information

The main idea
of the FW method is simple: to generate a sequence of feasible iterates by moving at
every step towards a minimizer of a linearized objective, the so-called FW vertex.

Learning Rate
appears to be a key aspect to understand:
while the O(1/k) rate in the original article was proved to be optimal
when the solution lies on the boundary of the feasible set (Canon and Cullum 1968),
improved rates were given in a variety of different settings. 

The slow convergence
behaviour for objectives with solution on the boundary,
moti￾vated the introduction of several variants.

One of the main features of the FW algorithm is its ability to naturally identify
sparse and structured (approximate) solutions. 
For instance, if the optimization domain is the simplex, 
then after k steps the cardinality of the support of the last iterate
generated by the method is at most k + 1. 
Most importantly, in this setting every vertex added to the support
 at every iteration must be the best possible in some sense,
a property that connects the method with many greedy optimization schemes. 
This makes the FW method pretty efficient on the abovementioned problem class. 

Another important feature is that the linear minimization used in the method is often
cheaper than the projections required by projected-gradient methods. It is important
to notice that, even when these two operations have the same complexity, constants
defining the related bounds can differ significantly (see Combettes and Pokutta 2021
for some examples and tests). When dealing with large scale problems, the FW method
hence has a much smaller per-iteration cost with respect to projected-gradient meth￾ods. 
For this reason, FW methods fall into the category of projection-free methods.

Furthermore, the method can be used to approximately solve quadratic
subproblems in accelerated schemes, an approach usually referred to as conditional
gradient sliding

in first order methods
maximal stepsize αmax possibly dependent from auxiliary information available 
to the method (at iteration k, we thus write a_k_max),
and not always equal to the maximal feasible stepsize.

Frank Wolfe classic algorithm:
choose x0 in C (set of feasable points)
for k in iters:
    if converged: break
    compute s_k with LinearMinimizationOracle
    set direction d_FW_k = s_k - x_k
    set x_k+1 = x_k + a_k * d_FW_k
end

Stepsizes:
- unit stepsize: a_k = 1
- diminishing stepsize: a_k = 2 / k + 2
- exact line search: a_k = min(argmin(f(x_k + a * d_FW_k))), for all a in [0, a_k_max]
    basically we manually choose for each iter the stepsize which minimizes our f (objective function)
- armijo line search: don't have a fucking clue about what it does or how it does it
    "the method iteratively shrinks the step size in order to guarantee
     a sufficient reduction of the objective function. It represents a good way to replace
     exact line search in cases when it gets too costly."
- lipschitz constant (L) dependent stepsize: no clue either.. what is even a lishcidg constant?
- there is also a strange lower bound of how much the step taken can improve our f, used often for convergence analysis
    (uses L and other weird shit)

FW properties:
- FW gap G(x): max(for s in C)( - d(f(x)T(s-x)))
    often used as a measure of convergence (way more complicated than this in the paper)
- O(1/k) convergence rate for convex objectives
    O(1/sqrt(k)) if non convex instead
    
FW variants:
- Active set FW variants aim to improve the O(1/k) convergence rate and ensure support identification in finite time (whatever the fuck this means...)
- we define for these variants:
    - Away step v_k = argmax(y in A_k)(d(f(x_k)T * y))  where A_k is the active set, subset of S_k
    - Away direction D_away_k = x_k - v_k
- Away Step FW chooses each time if better to take d_FW_k or D_away_k:
    - d_ASFW_k = argmax(-d(f(x_k)t*d), for d in d_FW_k, D_away_k)
- Pairwise FW simply takes both  d_FW_k & D_away_k:
    - d_PFW_k = d_FW_k + D_away_k = s_k - v_k